# Ansari Session 1 - Take 1


## [00:00:00] Intro to Ansari Backend

[00:00:00] okay, hello everyone. This will be quick overview of, Anari backend, repo, how it works, and, the main logic behind, the structure and certain files that we have and the naming conventions and all of this. Supposedly this will be put in the README file at the end. So, here in another monitor I have the, uh, topics that, I'll discuss today. And we'll go one by one through them and then we'll see if there's anything that, was left unsaid.


## [00:00:28] VSCode Global Search

[00:00:28] So the very first thing is a [00:00:30] little bit weird. It's outside of the scope of what we're seeing right now, is literally just a tip. Just before we get started, about, vs. Code, the IDE that I'm using here today, we probably know this, but, there is a functionality called the global search. you, uh, type control shift f in windows. I'm sorry, I'm not a Mac user. and that will make you that will allow you to search for the, for a specific term that you put here in the entire repo and tell you the files, uh, that have this term and where it was set, and you can click on it as you are seeing here. [00:01:00] This is very useful, very. And I'll be using this a lot in today's video. I just thought I'd, I'll tell you this. Um, 


## [00:01:07] VSCode Page Navigation

[00:01:15] and another cool short shortcut is Control P for me to go between different pages. Okay. So this is quicker than viewing the explorer from here and collapsing and seeing the structure from the start and navigating to where I want to go. So control P is a lot quicker, but that's when you get a feel around the project structure you'll be using this, these, shortcuts more.


## [00:01:29] Project Structure Overview

[00:01:29] Now let's actually [00:01:30] start to the start talking about the project itself. Uh, first I'll be talking about the general structure of the repositories folders. Right here, the main folders that we have, I will just briefly go through, through them. 


## [00:01:43] data/

[00:01:43] And so we have the first folder called data. It doesn't have anything else but ma folder. And, data here is initially just, a folder where we get data specific to specific tasks, but since we, we rarely use this folder, it's only, it only [00:02:00] has this file from the and you'll see some inconsistencies in the structure here. So you can, after this meeting, you'll probably s ee a function that can go to utilities. As you see later, there is a folder code utilities, and you're probably right like, there can be some reordering done to the project to make it more to make it easier to understand. But this is the general structure for now. I'll probably pin this recording to a specific version of GitHub. This, if someone wants to [00:02:30] trace, what I'm doing here with the current, with the version that I'm using right now. So it doesn't have anything else, just this function. And we'll see what this function does. Later. I'm just going through the overall overview of of the tasks, functionalities of the full functionalities. 


## [00:02:44] diskcache\_dir/

[00:02:44] The, this cache directory is just for caching. You won't see that, like it's just, is there when you run the code, but, it's not really important. It's just internal code by the, this cash library. And so we'll go to that later. Uh, 


## [00:02:56] docs/

[00:02:57] we then have, the docs folder. And, these, [00:03:00] as you probably know, is for documentation related, stuff in our project. And, We'll, we'll probably the recordings only just have the list of chapters that I'm reading from right now, the topics that I'll talk about. But we'll see where we'll actually put this recording in the end. And then the structure of API responses, this is a, okay, let me explain this. Specific parts of the code I am receiving or sending data, that you don't really see its structure in the code. And then if you see these [00:03:30] JSON files, it'll be easier for you to, to have a mental model of what we are dealing with in the code. So, for example, this is, a first, API request received from zrok. You'll see that I am very subtle in my naming conventions in the files. Any long name then, that's made by me. so so as you can see here, it's a bit long. So as literally the name says if you have a first request that's received from a I'll explain what this is in a second. Then expect a [00:04:00] structure like this. I literally, to get this, I printed it out in the terminal and put it here. I abstracted some info here this was actually an object in the terminal. So I said that this refers to an object instead of putting its elements and et cetera. This is useful just for you to have a mental model. You, in this request object, you can say request dot scope or request do headers and you'll know what you expect, the structure of what you're expecting. I think this is this normal info that anyone who used the fast DPI knows, but I thought I'd write it and put little [00:04:30] explanations here nonetheless. And then you have these other files. These are related to WhatsApp logic. We can get to this later, but essentially when we were dealing with Meta's, API to to get the requests from users on WhatsApp, sorry, WhatsApp we get these requests in this format. This is based on meta documentation, but meta's documentation isn't really clear to be honest. So I just got the request and printed it again internal and put it here. Yeah, so this again, to give you a middle model and any [00:05:00] any unique stuff I abstracted it into, uh, these delimiters, the this double Z and double more than that I've hardcoded this here. Um, And here, for example, I'm seeing possible values for this key.

[00:05:13] So this can be true or false and et cetera, like you understand when you see this file. But I, I really just wanna skim over the folders just for you to get a gist of the layout. Like layout of the land, of this repo. And these are a similar files. So the difference here is this, when you get a [00:05:30] reply message you from the API of WhatsApp, you, it'll be, it'll have these keys and, and this is when using zrok like this is when the request comes from a zrok, not from the actual WhatsApp server. Again, explain And this is for the final user message. These are really detailed based on the on the code that I'll show you later or the WhatsApp code. So I'll neglect like full explanation of them now, but when you trace the code there, you, these will make sense for you. You'll know which one is rated to which, uh, [00:06:00] logic. And finally we have this notebook file. This is it to HGBT. So how we receive the completion objects. So when you send a request to an LLM model, you you expect from from the the from the M provider, a response object. So this response object is what I have stored here. I, again, printed internal and I have it. So this is a model response object. This is a python object, which contains these attributes. Idea and choices. And choices is a list of streaming [00:06:30] choices and this is another object and et cetera. I did like I, I also took that from the terminal and pasted in here just for me to mentally visualize things.

[00:06:38] This will be useful when we go to the to an, sorry, the file. Like any file that does the coding of the actual element done in these people. And here you see yeah, an explanation of each attribute here. So, The possible values, which can be mentioned here and here. So for example, the content.

[00:06:59] What is this? It's the [00:07:00] content of the junk message itself. But, so what is a chunk? It's based on if you're doing streaming or not. So if you're not streaming, then this will not be a chunk, it'll be the whole message and all of these details, if you read this file one by one understand I'm very explicit in explaining the contents of this file.

[00:07:15] As you can see, again, any long names or long explanations, and you know that I'm the one who did that. Um, This is related to talks. I just wanna make sure that I'm following the plan properly. Yeah, explain docs. [00:07:30] Great. Okay. 


## [00:07:31] sql/

[00:07:31] so next we have the SQL files here. And this is this is a tricky part of this. So basically what we have been doing since the start of this project is that we were writing the SQL code with using the postgre sql, by the way. So we're writing the SQ code to create the tables. And whenever we make changes, we don't alter the initially written code. For example APO might do the following. They might [00:08:00] make a file called DB setup sql, and it just have the SQL command. And then whenever we make changes, like for example, we. Let's say for example, later on, I decided to change this first name to first username, for example. So I wanna change this column name, then some repos.

[00:08:18] What they do is that they change, um, the code here in line eight and change it to first username. And then we do the changes ourselves in [00:08:30] the in the production. So if you have the production server running right now with this table created, then. We go to the the production server, and then we do a, an alter command alter first name and to be alter username first username. And then here when we update this script, we just change this to first username. Okay, here, we don't do this. We don't do this social. Then What do we do? We do incremental changes as you see here. So 0 1, 0 2, 0 3. That means that at some point in the project, [00:09:00] we did this change. And then in the future, if we wanna update something, then we create a zero two file. And then we say what we updated. And this has an advantage and disadvantage. An advantage is that we know exactly how we, how, like you have a history of how we changed the database, our database, like what exact commands that we did. So we ensure successful replication each time. But the disadvantage is to make this replication, we need to run all these So if I were to right now [00:09:30] instantiate a local database in my, in my laptop here, then I'll have to run these files one by one. This is a disadvantage, but this is how we did it and we stuck with this logic for now. Um, and this logic is useful for you to know. And another two files, actually I will. I'll go buy them really quick and then get back to them later. Uh, called set up database and Python app. So again, I'll go to them again, [00:10:00] but just to, to tell you why this logic is important and you can just change this logic because changing this logic, uh, will employ you change these files as well. So instead of database, it's just a file that we made that if you want to set up this database locally, then you can run the script and it'll import the files here. Like it'll run this passing the SQ l directory, which is sql, which is what I shown you above here, SQL the DB URL which is in get settings.

[00:10:26] I will explain this later, but in short is just a, it has [00:10:30] this default value, so it'll set up locally with these URL. Uh, And it'll create it and we'll create it via this function, which will go through, iterate over each file that ends with SQL. See, so so if we are actually here, because I was about to do this at some point I was about to, for example, to create a an SQL here called an aggregated or all commands do sql, for example, which is just me concatenating all the SQLs in this file and this and this into one file.

[00:10:57] This for me to easily visualize what we did, but no [00:11:00] don't do that because, doing this will will affect other files like the ones I just show shown you. It'll be iterated over and the commands there will be run as well. So if you'll do that, create another SQL folder, SQL Dev or SQL temp, whatever, and then do any changes that you want.

[00:11:16] But here in this folder, only increment do not change the previous only increment. When I add WhatsApp features I created a new script with what I want to increment the live save. Okay. The other file, which [00:11:30] also uses this logic is Python app, cml, and this is the CICD that what GitHub action uses when we push any commit to GitHub. Um, and so we won't go too much detail here, but I just wanna show you this line. This line, create the tables by running sq skips. So it's one of the jobs that that the CICD pipeline does, is that it creates the tables by going through a full loop in the SQL file. So it is a [00:12:00] sq l folder. So this logic is applied twice. If you want to change this wave of logging or tracing our how we write sql, then we have to take into consideration these two files as well. Okay. Uh, this is regarding SQL Folder explanations of sql. I don't think I really need to do that. Like they are straightforward. Now you should, that you should be aware or it should make sense to you why we did a script called Alter User Tokens with just these alter [00:12:30] commands.

[00:12:30] Like any newcomer could say, wait, but why don't we just, uh, do not here, like a drop constraint. Okay, so why did we put it in the first place? We can just remove it. In the first script. Sorry. But no, we don't do this because the logic that I just said. It's we apply things incrementally. So yeah, I don't think I need to go in depth of the explanations of here.


## [00:12:49] DBeaver Tool

[00:12:49] There's a nice diagram that I that I created which, visualizes the schema of the database that we have. This should be up to date. Uh, with with what we, what we've done. Obviously you see [00:13:00] here dependencies and triangular depend. This isn't the best way to model things, but that's what we have right now and it works. And, there could be, we could refactor it soon. but, the way that I did this is optional info by the way, is using a tool called the DBeaver that I have here. it's, uh, free and it's, in my opinion, the best IDE that I used for managing stuff. and so there was, just a button to this diagram automatically, and you can maneuver it around. [00:13:30] There's a lot of solutions online, but this is the one that I used. Um, okay. So that let it open in the backend and take its time. And for now, I finished the S QL port. Hopefully I remember to say everything about it. Okay, so that was finished in nine 50 here and next should be the source folder, but I'll skip this folder for now because it's, it has the bulk, the meat of the project. I will make this, I will let this take its sweet [00:14:00] time and we'll move instead to test folder. And so this was nine. So by the way, this is, yeah, this is DBEaver and I, yeah, because here to create the database, I actually didn't, I didn't I didn't run the com the file, which is called set database. I didn't do that. I actually just pasted, like I took all of the files here. And just pasted them one below the other and made this naming like this was the first file . Pasted and [00:14:30] this is the second file. And I didn't just ran all of these and so it's not the best way to do things, but hey, in world. Yeah, obviously that was before I knew about the setup stuff. 


## [00:14:42] tests/

[00:14:49] Okay, so back again to the test folder. So the test logic, we again, I just came through this because there isn't really much it's uses.

[00:14:53] We are using pytest library here, and in each of the main files in the repo, you'll see me add like [00:15:00] couple of comments, python comments to explain the overall objective of this file. Obviously, you don't have to read all this. If you are one who understands better by reading the code, then skip that and read the code.

[00:15:09] That's why I put it in the start so that all of the big old comments which explain the overall reason of this file and what it does and all that, I put it in the start. Because if you wanna skip that, you can skip that instead of just putting them amidst the code here. So what is amidst the code here as just few lines, not overall logic. And any [00:15:30] technical notes or tricky notes or explanation for those who don't know, I leave that at the start. So again, literally from its name, this file aims to test the main API endpoints of the app, which is this file. And this is in the source folder. I'll go to that later. Using pie test library and first a p discount. Um. The steps here. Literally all of this was like JT generated, is just, we go through the code here and it just gives an overview of what we are currently testing. Oh, there is a disadvantage of this is that whenever we make changes in the [00:16:00] code if we, for example, remove a test entirely, then we have to remember to remove it from here as well. So this is also a disadvantage, this method but at least it decouples the comments a little bit from the rest of the code. So yeah, like anything else there, there are adventure and disadvantages, but for now we are leaving this as is. Maybe in the future we'll make a dedicated documentation file, which store these all of these comments in all these files.

[00:16:24] And we refer, like we can say here refer to header ethic for [00:16:30] this specific info and et cetera. But for now, we're leaving this. Okay. So these are the general steps. Uh, I really don't think I need to go through them. If you read them in your time, you understand that. And literally some of them are too verbose because again, these are a AI generated, so you have important, necessary modules and you can see that. So there are some details that we don't need to go through. I, while I was making these comments, I was thinking of what mi like a newcomer who really don't know the tech stack of what we're using. [00:17:00] need. For example, if someone doesn't know py test that much, what can I say here that can help them understand this new tool? So, When I read the code, because I really wasn't familiar, even myself, I wasn't familiar with PY test before this recording. But when I read the code, it's understandable. Even the naming conventions are clear and so I know what this test aims to do. And these are comments saying it.

[00:17:23] So there isn't really much do, but for example, this decorator, I don't understand this. Another decorator from [00:17:30] markers, I didn't understand that as well. So stuff like this, which I think isn't really understandable at first glance, even by newcomers, stuff like that. I put it in comments. So here I explained, and obviously you can see websites which explain this as well.

[00:17:43] I don't have to put all the explicitly here, but I try to give a gist of what's happening. Instead of you seeing websites, which sometimes overcomplicate explanations. I'm not going to dive deep into like technical details of how pie test works. To be honest. It's not the aim of this recording, [00:18:00] but, hopefully this main comment block is enough.

[00:18:03] And obviously, in these technical notes, I put an optional here. Uh, or between brackets or anything just to indicate that this, you can read that if you think that's worth of reading, but if you know that info already, you don't have to read it. And normally when I put this note as well in capitalization, it is probably also a technical note. Like these are just general steps of what we're doing here and, why this file is created. But this is like technicalities, like for example here, if you didn't understand what is a test client [00:18:30] in, and I'm explaining it here, how it internally works. So that you know that we're not internally creating a server, it's just emulating the cre, the creation of a server that's not creating one.

[00:18:39] So maybe that info is useful to you. Yeah, this is, I think enough for the testing file here. Is another one, for the quality. This one specifically is just to see that the responses from the model that we use is currently T four oh. Is good enough? It's good enough. How do we decide good enough? Uh, it's [00:19:00] based on me on data sets that we have. So, For example, we ask it a question here and, based on, and give it some options here, separated by comments and give it the correct answer. And so in distance test and quality file, we take this CSV for example, and, we split the, columns here. We give it the questions initially and we give it the options, and we tell the LLM to answer based on the solution that we made here in the project. And so if it [00:19:30] answers correctly, uh, we see how many questions answer correctly, and based on that, we give it the score. Obviously, the more data dataset we have, the better, and we have a dedicated deficit for this.

[00:19:39] I believe. Like I, I think there are other data sets that we tested on if you go to, I'm sorry. Okay. Project itself, it's itself Here, you'll see I'm sorry that this says, yeah, there is here the, we just gave this 12 months ago, I believe. Yes, five [00:20:00] months ago. And I'm not sure if the other data is here.

[00:20:03] We use it in the code right now, but CSV one is the main one that we use. This is also for you to know if you. We would like to contribute, um, in the evaluation part. This file, I might explain it. This file is called, jinja template. Syntax is and, the test files here use this formatting, but replaces the question and options, uh, with what we have from the CSV here. So, Okay. So [00:20:30] how do we find something like this? This is actually a good test for what I explained to you previously about the control shift F part. So if, for example, say you're a newcomer to the project and you wanna see how is this file used? Okay? Might control p to see the available pages with this name to ask question, and you will not see anything except this file.

[00:20:48] Okay. Good enough. This will not work. Let me try control shift f This will save the content of the of the files. And see, where is this name mentioned? So if I say, ask [00:21:00] a question, okay, I do. I find it here. I find it here, which is the disc and quality. So that's actually how I myself knew that before doing this video. And we see that it's used here. And we see that it uses tn which is based on jinja here the thing that I told you about. And so it, it reads the question, the template from here and based on it splits it from the CSV. Uh, so it is this tool, this shortcut [00:21:30] control shift F is very useful.

[00:21:31] The global search feature. Um, this is something tricky note though. I actually searched with underscore here, even though the file's name is. Ask question with a dash. So if we do this here, we actually will not find it. We won't find it. We'll only find here in, in a do egg file. This this is, this file isn't necessary and I won't go into this explanation.

[00:21:53] It's actually auto originated, is useful for us. So ignore any dot egg files, their uses for us. If you do that [00:22:00] with Dash, like it's naming here, we won't see. And this is a slightly tricky part about the global search method of finding things is that sometimes you'll have to take some things of what you want to search about and hope it matches with something. Um, Sometimes it's the case and sometimes it's not. Again, you don't have to always do this solution. Like it just depends on what you're trying to search. Because I'm searching for a name here for a question here. Then obviously vs. Code won't help me. Like vs. Code won't give me what's referencing [00:22:30] like it I can't control click this and see previous references. Uh, but in something like data here I can do that. So if I control click data, I can see what it was mentioned in this file. So it was mentioned here. So test, I'm sorry, agent relies on the data function to get the data here. Yeah. So the control shift f is very useful when it's tricky to use symbols. Okay. To rely on symbols that vs code can can reference for you. Okay. I'm sorry, I went a little off topic here, but because we really [00:23:00] should see the fun, the features of your id because it'll really help you down the road. So yeah, I think I finished the test spot. Yeah. And yeah.

[00:23:11] And now I believe I finished the folders, the explanations of the folders. So these are all the folders. O obviously I'm leaving source for the end. This is the hierarchy of how we're working. 


## [00:23:20] .vscode/

[00:23:23] Obviously the, those vs code file is just a, an internal folder that have the sittings based on VSCode here. I can I, I am the one who wrote these files. [00:23:30] These are in the GIT ignored file. So I here put extra settings that I find useful for me. Uh, but you don't have to follow the rule and I will not explain what these things do because that's out of the scope, but you don't need them.

[00:23:42] If you don't want them, you will not need them. So yeah this, I really shouldn't explain this. And yeah, here we finished with the purpose of reus folders and now I'll explain the purpose of root files. So we start this [00:24:00] on oh, 10. 10. Ah, hopefully you are not bored by now. Okay. So the purpose of root files. So before going to source, I will explain the root files over here. Um, explain some of them at least with the order that I have here. I will skip end of example for the end. Because it's, that's the most important one. And it's heavily tied with source folder, 


## [00:24:28] .python-version

[00:24:48] the dot Python version. Okay, this is a tricky one. Previously, if you see any previous commits, you will see that this file was called runtime dot text, but we changed that. Why did we change this? Because of the following. These are really extra info, but it may help you as well in other projects. So why not? We are using Heroku. Uh, Heroku server, a Heroku service to deploy our server to that platform in order for it to appear to all the users that we have. Heroku has some [00:25:00] guidelines and some announcements that come every now and then. One of these announcements is that the runtime text file is duplicated. What the heck is a runtime to text file? Runtime to text file? Let me see if they're actually explaining it here. Informations. Okay.

[00:25:17] So basically it tells platforms like these, like Heroku. What is the Python version that you're currently running? It? This is basically what it does. So, If you see here we have it, we were running this, but [00:25:30] this is for the server like this. The server of Heroku, which will host the application should run on this. This is what I understand, but I'm not sure about this though. Uh. Or maybe this refers to the actual application that we have itself, it run on three point 13. Maybe this is the case as well. I, even myself, I need to recheck this. Essentially it's just a configuration that tells Heroku which Python version to run. Okay. And, It was previously called runtime to text, but we, but they decided to change this. And this is [00:26:00] actually new news. Uh, And the effective change will be on 6th of February. Yeah, as you can see, we should have changed it. And so I did, even before this recording, two Python version, they are the ones who suggested this suggested, so they said, here we should make it a do Python version file.

[00:26:17] And this makes more sense to be honest, because that's literally its objective. It's just to tell the Python version. So it makes sense to, to enable this, okay. So this is the use of this file. This literally shouldn't have taken me more [00:26:30] than a minute, but I don't know why I took a long time explaining it. Uh, 


## [00:26:34] Procfile

[00:26:37] There is also a profile here. This is also used by Heroku. In order to, okay. I wanna explain this, Just rather than I explain it and, missing any important detail, I will literally just. Say the, it's exact use in front of you. This is the best article that I found explaining it. This one, it's literally just a starting point for any app running on hero. So, Where you declare one or more processes to be run for your [00:27:00] app to function. So we first run the commands here before our application starts. Our service starts in the deployed HEROs. Okay. So for example, you, we can say the following, what are the commands? What are the processes that we need to initialize before we start turning our server? Okay, so this is an example, just echo example. So if you see here, just, let me to mean each one here just writes an echo. If you [00:27:30] run. This profile you'll see in the console, in the logs of the Heroku environment, uh, website. You'll see in their logs something like this. Echo spinning up the web process, which is this one. And the normal works of booting, which is this one and this search. So, Here we're using only web, I believe. Recommend. Yes. Web here. And we're passing these. So what are we saying here? We're basically saying we want the Python path to be this. Uh, which is the current Python path, the [00:28:00] variable that we have, object that we have in the environment, and source to put the source folder, which is this one. So to put source in the Python path, this is what I understand from this. For us, if we make imports and et cetera in the main Python files, it'll, it'll be understood.

[00:28:15] Like they'll understand what source is. Okay. And to use a unicorn server number of workers is for, I don't really need to go into depth with but this is just saying to Heroku how we should configure [00:28:30] the environment variables and the infrastructure of the server, which is running our deployed application. Okay. Um, and any extra configurations for. So this is the use of this file. We don't, we rarely touch this file, so a brief explanation of it is sufficient. 


## [00:28:47] pyproject.toml and ruff

[00:28:47] Then we have l file over here. this contains configurations for the, overall project. There are a lot of, documents online explaining, what you [00:29:00] can achieve with a by project to two five. So I won't really go into depth here. And there are info that you can, you can just even without reading something online. You can deduce that. Okay, this is the file that, allows me to put metadata regarding the project, like this one page and documentation, et cetera. But what I want to explain out of all of this is this part. So we are using a linker called ruff. What is the link here? It's basically when we're writing code, we want to abide [00:29:30] by certain practices. And so which practices do we abide by? Are a lot of options online, a lot of guides, a lot of guidelines. Uh, so we use guidelines related to ruff. Okay. So even if I, 'cause to be honest, I don't really know how to, say its exact definition. I will not get anything. Okay. Now I want the tool, I want the tool for this. Yeah. A static code analysis tool designed specifically for Python. Okay. And we are using it, [00:30:00] yeah. Again, for speed and efficiency. So when we get ruff here and we install it here, it'll efficiently and quickly scan the code that we're writing and say that, wait, you are not following our guidelines in this specific principle. What are their guidelines? One of them is, for example, that we have put here in this repo, is that we can't exceed a certain amount of characters in our repository, in our entire repo in a certain line. So each line would have a maximum of, if you point [00:30:30] here, 1, 2, 7 characters. Okay? So here you see an issue code line to long because it has 141 characters and you're only allowed to have a maximum of 127. Okay. And it's saying what the guideline is, like this is a guideline ID, which specified this. I, so if you open this it'll expand it here. Okay. The preview pens of the ID are really important, so it'll help you out a lot. So here it explained it, or it does checks for lines. It exceeds specific maximum character length, so you [00:31:00] understand it now. So an important remark about this is that in the CICD pipeline, we. Stop. We, we say that the job failed and that the code that you're trying to push to our project failed if it doesn't meet the guidelines of ruff. Because some repos don't put that hard deadline. No, we put that, that, not deadline, sorry. That, that, defining line, like a strict defining line of how to write code. We have strict rules here. Okay. What define [00:31:30] this rule? Like how do you know these rules? In the by project file? So here you'll see line length is 127. Okay. So if I change this to 130 for example, then I can add here three more characters. Wow. Are, uh, some other, stuff that we also add as, guidelines here. And there are guidelines that we ignore. For example, the, if we put in the list here, these guidelines, which again, you can see a list of guidelines in the official website to, to see which Id you should put for [00:32:00] which logic that you want to ignore.

[00:32:01] This list will be ignored. For example, this, guideline says that for any function that you make. You should add doc strings like documentation. Okay. If I recall correctly. And, And so putting it here in the ignore means that no, we can have some functions. Let me actually remove this. We can have some functions like this one actually, which doesn't have a doc ring. Okay. Which doesn have a dock string like this, which explains what the function does. And so these are other stuff that we ignore. [00:32:30] Okay. And you can continue on the, with the list. Okay. But what do we have? What if you have an exception? What if you, for some reason we want this specific comment to exceed the, or bypass this guideline. We point here and in the quick fixes disable for this line, which we put this, putting this will make mayr when it checks the code. When it passes by this line, it'll say, okay, so this line is an exception. So I will not run my analysis on the line for this guideline. Okay. So this is also an [00:33:00] important thing to remark because I think we used this in, in, in the new QE research.

[00:33:04] Yeah, we used it here. So this actually failed with me in a previous commit where we had an issue in the live server where where this specific clause didn't work. And so we commented out here. So when we commented out here, you will see if I un comment, if I un comment this, you will see this. Wiggly is squiggly yellow underline saying that, is assigned. This variable is assigned but never used. This is a rough [00:33:30] guideline that we just broke right here, and it's, this is, it's hyperlink. And so if we deploy this right now as is to GitHub, it'll fail the CICD, which I showed you previously in here in the Python piano. Remember that? Yeah. We have another one right below it about the linting Stop the build if there are lint errors. So it checks, it does a rough check command based on the applied project to file. Just I showed you and, makes the output compatible to GitHub, [00:34:00] to, to show on the GitHub actions as I'll show you later. Okay. So yeah, this failed. This actually failed on life on the main branch. That quick and easy fix for this is to either remove this entirety and move this entirety. And I do not wanna do that because we want to return that when we fix things. Or I can simply put this new qi. Okay. Put this no check. And, Again, highlight, quick fix and disable for this time. Okay. Okay. Uh, [00:34:30] what else do we have? This is for the pipe project tool. One final thing regarding ruff, you can get the VSCode extension here because again, these squiggly lines, how do they appear to me? They appear because I installed here in extension vs. Code, code graph. Wait for it. It'll appear here. Okay. Yes, it's okay. You should insert and you should have it activated and one way. To see this, that it's activated, is that you can see [00:35:00] is it because I'm zoomed in? You can see it one second.

[00:35:02] If I zoom back. Yeah. You can see here in this part, if you highlight here, you can see that ruff is working here. Okay. It's log to make sure it's working. I'm saying this because sometimes it doesn't work and I will not say when these times are. I will just point you to an issue that I made. I actually don't have a sink now.

[00:35:21] So either way, these source can be beneficial because if is not working, then you will struggle a little bit in this project because if [00:35:30] you accidentally bypass or don't abide by one of the guidelines of ruff, and then you commit and then you push. You'll be surprised with a GitHub action saying that no, the job failed because they didn't abide by the rules. So obviously it's better for you to know them in advance before committing and pushing to GitHub. So this extension is important. Okay? So if it doesn't work, then you know where to go here, and you normally open up an issue in the ruff um, repo on GitHub and. [00:36:00] State to the developers the issue that you faced and these docs here, right here. Yeah. But hopefully you've not encountered this issue. It really happens. So this is all I wanna say regarding the BI project tool file. And ruff, I believe I don't have anything else to say in it. These are all specifications you can eat about them in your own in your own time. But you really don't really even need to understand all of them.

[00:36:24] It's just whenever you break one of the rules, you will see an indicator that says that you broke them. And [00:36:30] vs. Code, God knows vs code really really shows that three. So if you did that, that the entire file here is marked in yellow and yellow here. And even this port, if you can see the screen like this port in the right. That has a mini map of the of the code. It'll show you the line which have the issue. You can point it here and you can see that. You'll You'll know. So don't worry about that. This is all related to the Point project. I'm sorry that I took a long time [00:37:00] in this, but hopefully it's clear for you now.


## [00:37:04] pytest.ini

[00:37:04] The file. Okay. So we have next, where was it actually? I don't, I can't see it here, so I will use my trustee shortcut. Yeah. Okay. Okay. I will I can redirect you after this video, in the video description or whatever to resources for understanding the usage of this file. But to [00:37:30] summarize, it just sets up like, how can I say it? Configurations in order for the Python test files to abide by for for example, we define a marker here called a sync io, and we give the description. Okay? So now if we go here in, in Cayo. Here, this will be understood by the Python test framework when you're running the script, because we define here, we define it here as well as the Python path.

[00:37:59] So here, [00:38:00] if you remember when I say Python path pass no wait. I think this, yeah, this automatically the Python path, for src. So what this does is when we run this script using Python Library , in the environment where this script is running, it'll automatically put the source into the list of environment variables, the Python path file. You can see a detailed explanation of each of these keys , in the articles that I'll post with the description in this [00:38:30] video. We rarely again, modify this file. So just knowing that it's related to the configurations of the pie test files is good enough for now. 


## [00:38:39] README.md

[00:38:39] Then we have the README file where then we have the Readme file over here.

[00:38:47] The Readme file is a little bit outdated, to be honest. Just a quick shortcut. You can, in vs code, click on control shift v. To open the page in a preview review mode. Uh, so just a [00:39:00] little bit cleaner than the markdown equivalent. And I know I should have started the recording with the README, but I did because there are also some other stuff in it. We'll add a link of this according to the Readme when we are done. Because it'll be more up to date, so I don't really need to go over its content because that's the whole purpose of this video. 


## [00:39:20] Markdown For All Extension

[00:39:20] Just a quick tip, I can say, at least regarding this port, there's a quick way to auto generate a tip of content based on the markdown headers here.

[00:39:27] If you install a library called you installed that. Okay. [00:39:30] And then you open the Command pallete in this code, which is in control shift P, which open this here, you can see table of content and here create table of content, uh, which when I press it, it generates, its this for me. And now when I update any of this and control s it gets updated here. This is also useful for documentation purposes. Okay. And, now for the. Requirement of text file. I'm going by this [00:40:00] project bit by bit. Hopefully you are not bored from, 


## [00:40:04] requirements.txt

[00:40:04] requirements of text. So as you can see here, we don't put the entire , requirements like the, we don't pin, we don't pin the requirements, we don't put exact here. This is just for, easy upgrade purposes to easy upgrade things. And if we see issues. Between certain dependencies, we can pin versions. For example, this we pin this to these range of versions, I think because, version 2.0 could [00:40:30] introduce, breaking changes in the API code that we have, uh, in the inpo itself where we used, this library. And so we don't want that and so we limited this to that. To that to that scope only. Uh, how do you know? You don't really, until you see stuff breaking in the life. But overall, overall, you we don't use pinning we use exact version in here. And so based on that, we don't use pip freeze commands.

[00:40:59] So the pip freeze [00:41:00] command, it just tells you based on the environment that you have opened, the version environment that you have opened, the list of all the commands that you have, list of all the libraries that you installed, and with it pinning with its versions. But we don't want that ity.

[00:41:12] We just wanna state which libraries we actually ran the command pit installed on. Okay. Because the pip freeze commands it, it says these libraries that we installed and any other libraries, which the. Our installed library depend on. So if I installed [00:41:30] Pandas, for example it installs it installed NumPy in the background.

[00:41:34] So you'll see, I believe, if I remember correctly, so we see MPI over here, even though I don't think we installed, yeah, we don't have PY here explicitly. If I, yeah. We don't wanna, we don't wanna make this list exhaustive. It's just for what we installed ourselves ourselves. So that means. Whenever we install a library in the future, if we are going to make any new feature, then we have to remember [00:42:00] to put this install library as it's written here, we put it in its corresponding place in the requirements text. We do that manually. Okay. And sort it, preferably sorted, won't matter. This are a quick shortcut for short for sorting control shift p and then you say, sort find ascending and it'll sort them ascending. I'm sorry I go off topic a lot of with these tips, but they are useful. 


## [00:42:22] setup\_database.py

[00:42:22] Now for the setup database file. Wow. We're back. We're back with this file again. So I explained this before, [00:42:30] but again, basically this is just, uh, for you to be used by you, the developer locally to create the meeting tables in your local database. Uh, or the missing database in general. If you don't have database instance, you can start with this. Okay. But to be honest, I didn't try it myself. Like I myself did. The old fashioned method that I showed before showed you before is just copy and pasting as you see here. Uh, I think there was just a remark if for some reason you wanna follow this method instead. Uh, I think it's just something that I [00:43:00] like a specific thing that you should comment. Yeah, here in in the in file four. In file four, which is this file, sorry, this file create type uh, if, okay, if you want to redo, database changes for example, you create a database instance and it has row in the table and you wanna wipe it all clean and truncate everything or remove it and then. Redefine the tables again for some reason. [00:43:30] Then just be careful that you should comment this in any later reruns, uh, because it'll give you an error. It'll say that it'll say that the type feedback class is already defined because it is defined. I don't know if I will be able to bring it here, but it is defined in another port.

[00:43:48] Then the ports where the scheme is, are defined, I think in. Rules, not rules I won't go with now, but it, it remains in the editor that you're using and the idea that you're using. So it'll give an [00:44:00] error. So in future res you can comment this out. If it's your very first time running this, then you can leave it in. Okay. It's just a very quick tip there. If again, for some reason you wanna copy paste all the files like I did, instead of using the setup, the file. Okay. And now we're done with this file. It really doesn't do anything else. It just loops over them. And then logs that it imported the, this specific SQS script and then moves on.

[00:44:24] It really doesn't do anything special. and then [00:44:30] comes Ah, I need to drink water before explaining that part

